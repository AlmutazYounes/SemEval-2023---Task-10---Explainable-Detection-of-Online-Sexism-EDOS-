{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm"},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"173fe52379437b78f95c8980b8ee9f2930fd7b56889ab31a72735475ddc10c81"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"8ed424c0e01b4b43940a169526a722d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d7e7d0ca49a4677ba95f73dd0d293b7","IPY_MODEL_adb50d702ef7464ca4e763627ecc511b","IPY_MODEL_8523ae362bd34f408740b018d3565e8c"],"layout":"IPY_MODEL_4763edc471124faca55fc00f280fd523"}},"1d7e7d0ca49a4677ba95f73dd0d293b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8296b4f4e244ab4b65a17b7819d7560","placeholder":"​","style":"IPY_MODEL_4b0b7d1a62c94c2d953ace0038c943ef","value":"Downloading: 100%"}},"adb50d702ef7464ca4e763627ecc511b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_53acf063973f46c2906ddfedebc84f64","max":1344997306,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cec12c3a397245eca3df802baf249e9d","value":1344997306}},"8523ae362bd34f408740b018d3565e8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fb99a1e76904f72bf2662a62b4a8e97","placeholder":"​","style":"IPY_MODEL_4dc1853e74f14d20b9dbc511ed74a98a","value":" 1.34G/1.34G [00:30&lt;00:00, 75.5MB/s]"}},"4763edc471124faca55fc00f280fd523":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8296b4f4e244ab4b65a17b7819d7560":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b0b7d1a62c94c2d953ace0038c943ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53acf063973f46c2906ddfedebc84f64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cec12c3a397245eca3df802baf249e9d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2fb99a1e76904f72bf2662a62b4a8e97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dc1853e74f14d20b9dbc511ed74a98a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-04-09T04:50:37.263228Z","iopub.execute_input":"2023-04-09T04:50:37.263697Z","iopub.status.idle":"2023-04-09T04:50:37.308468Z","shell.execute_reply.started":"2023-04-09T04:50:37.263619Z","shell.execute_reply":"2023-04-09T04:50:37.307488Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/semeval2023-task10/train_aug.csv\n/kaggle/input/semeval2023-task10/dev_task_a_entries.csv\n/kaggle/input/semeval2023-task10/test_task_a_entries.csv\n/kaggle/input/semeval2023-task10/edos_labelled_aggregated.csv\n/kaggle/input/semeval2023-task10/train_all_tasks.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture c\n!pip install transformers\n!pip install datasets\n!pip install sentencepiece","metadata":{"id":"CUbqKF9Q9Vd2","execution":{"iopub.status.busy":"2023-02-25T17:23:11.621443Z","iopub.execute_input":"2023-02-25T17:23:11.622028Z","iopub.status.idle":"2023-02-25T17:23:40.165508Z","shell.execute_reply.started":"2023-02-25T17:23:11.621992Z","shell.execute_reply":"2023-02-25T17:23:40.164178Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom tqdm import tqdm","metadata":{"id":"bJ0Xo9Vt9VbI","execution":{"iopub.status.busy":"2023-02-25T17:23:40.168544Z","iopub.execute_input":"2023-02-25T17:23:40.168972Z","iopub.status.idle":"2023-02-25T17:23:40.174571Z","shell.execute_reply.started":"2023-02-25T17:23:40.168932Z","shell.execute_reply":"2023-02-25T17:23:40.173546Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def process_data(row, max_len):\n    text = ' '.join(str(row['text']).split())\n    encodings = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_len)\n    label = 1 if row['label_sexist'] == 'sexist' else 0\n    encodings['label'] = label\n    encodings['text'] = text\n    return encodings\n\n\ndef get_prediction(text, max_len):\n    encoding = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_len)\n    encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n    outputs = model(**encoding)\n    logits = outputs.logits\n    probs =  F.softmax(logits.squeeze().cpu())\n    probs = probs.detach().numpy()\n    label = np.argmax(probs, axis=-1)\n    return {'label_sexist': np.argmax(probs, axis=-1),\n            'probability': probs}\n\ndef get_probabilites_dev(model_name, val):\n    taska_preds = pd.read_csv('/kaggle/input/semeval2023-task10/dev_task_a_entries.csv')[[\"rewire_id\", \"text\"]]\n    taska_preds.columns = [\"rewire_id\", \"label_pred\"]\n    tqdm.pandas()\n    taska_preds[\"label_pred\"] = taska_preds[\"label_pred\"].progress_apply(lambda text: get_prediction(text, val[\"max_len\"]))\n    taska_preds[\"label\"] = [\"sexist\" if i[\"label_sexist\"] == 1 else \"not sexist\" for i in taska_preds.label_pred.values]\n    taska_preds[\"probability\"] = [i[\"probability\"] for i in taska_preds.label_pred.values]\n    taska_preds[\"not sexist\"] = [i[0] for i in taska_preds[\"probability\"] ]\n    taska_preds[\"sexist\"] = [i[1] for i in taska_preds[\"probability\"] ]\n    taska_preds[\"label_pred\"] = taska_preds[\"label\"] \n    del  taska_preds[\"probability\"] \n    del  taska_preds[\"label\"] \n    taska_preds.to_csv(f\"dev_{model_name}_maxlen_{val['max_len']}_batch{val['batch_size']}_lr_{val['learning_rate']}.csv\", index=False)\n\ndef get_probabilites_test(model_name, val):\n    taska_preds = pd.read_csv('/kaggle/input/semeval2023-task10/test_task_a_entries.csv')\n    taska_preds.columns = [\"rewire_id\", \"label_pred\"]\n    tqdm.pandas()\n    taska_preds[\"label_pred\"] = taska_preds[\"label_pred\"].progress_apply(lambda text: get_prediction(text, val[\"max_len\"]))\n    taska_preds[\"label\"] = [\"sexist\" if i[\"label_sexist\"] == 1 else \"not sexist\" for i in taska_preds.label_pred.values]\n    taska_preds[\"probability\"] = [i[\"probability\"] for i in taska_preds.label_pred.values]\n    taska_preds[\"not sexist\"] = [i[0] for i in taska_preds[\"probability\"] ]\n    taska_preds[\"sexist\"] = [i[1] for i in taska_preds[\"probability\"] ]\n    taska_preds[\"label_pred\"] = taska_preds[\"label\"] \n    del  taska_preds[\"probability\"] \n    del  taska_preds[\"label\"] \n    taska_preds.to_csv(f\"test_{model_name}_maxlen_{val['max_len']}_batch{val['batch_size']}_lr_{val['learning_rate']}.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:35:04.437051Z","iopub.execute_input":"2023-02-25T17:35:04.437448Z","iopub.status.idle":"2023-02-25T17:35:04.456347Z","shell.execute_reply.started":"2023-02-25T17:35:04.437414Z","shell.execute_reply":"2023-02-25T17:35:04.455122Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_csv('/kaggle/input/semeval2023-task10/train_all_tasks.csv')[[\"text\", \"label_sexist\"]]\n# df = df.append(dev).reset_index(drop=True)\n\nmodels = { \n\n\"debertav3base1\" : {\"model\": \"microsoft/deberta-v3-base\", \"max_len\": 50, \"batch_size\": 32, \"learning_rate\":5e-5},\n\"debertav3base2\" : {\"model\": \"microsoft/deberta-v3-base\", \"max_len\": 60, \"batch_size\": 32, \"learning_rate\":5e-5},\n\"debertav3base3\" : {\"model\": \"microsoft/deberta-v3-base\", \"max_len\": 70, \"batch_size\": 32, \"learning_rate\":5e-5},\n\"debertav3base4\" : {\"model\": \"microsoft/deberta-v3-base\", \"max_len\": 80, \"batch_size\": 32, \"learning_rate\":5e-5},\n\"debertav3base5\" : {\"model\": \"microsoft/deberta-v3-base\", \"max_len\": 90, \"batch_size\": 32, \"learning_rate\":5e-5},\n\"debertav3base6\" : {\"model\": \"microsoft/deberta-v3-base\", \"max_len\": 100, \"batch_size\": 32, \"learning_rate\":5e-5},\n\"debertav3base7\" : {\"model\": \"microsoft/deberta-v3-base\", \"max_len\": 110, \"batch_size\": 32, \"learning_rate\":5e-5},\n\n\"bertracism3_3\" : {\"model\": \"MutazYoune/bert_racism3\", \"max_len\": 80, \"batch_size\": 32, \"learning_rate\":3e-5},\n\"bertracism3_4\" : {\"model\": \"MutazYoune/bert_racism3\", \"max_len\": 90, \"batch_size\": 32, \"learning_rate\":3e-5},\n\"bertracism3_5\" : {\"model\": \"MutazYoune/bert_racism3\", \"max_len\": 100, \"batch_size\": 32, \"learning_rate\":3e-5},\n\"bertracism3_6\" : {\"model\": \"MutazYoune/bert_racism3\", \"max_len\": 110, \"batch_size\": 32, \"learning_rate\":3e-5},\n\n\"bertracism4_2\" : {\"model\": \"MutazYoune/bert_racism4\", \"max_len\": 70, \"batch_size\": 32, \"learning_rate\":3e-5},\n\"bertracism4_3\" : {\"model\": \"MutazYoune/bert_racism4\", \"max_len\": 80, \"batch_size\": 32, \"learning_rate\":3e-5},\n\"bertracism4_4\" : {\"model\": \"MutazYoune/bert_racism4\", \"max_len\": 90, \"batch_size\": 32, \"learning_rate\":3e-5},\n\"bertracism4_5\" : {\"model\": \"MutazYoune/bert_racism4\", \"max_len\": 100, \"batch_size\": 32, \"learning_rate\":3e-5},\n\"bertracism4_6\" : {\"model\": \"MutazYoune/bert_racism4\", \"max_len\": 110, \"batch_size\": 32, \"learning_rate\":3e-5},\n\n\"hateracism90000\" : {\"model\": \"MutazYoune/bert_hateracism90000\", \"max_len\": 90, \"batch_size\": 32, \"learning_rate\":5e-5},\n\"hateBERT\" : {\"model\": \"GroNLP/hateBERT\", \"max_len\": 90, \"batch_size\": 32, \"learning_rate\":5e-5},\n    \n}","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:35:06.290519Z","iopub.execute_input":"2023-02-25T17:35:06.290922Z","iopub.status.idle":"2023-02-25T17:35:06.304608Z","shell.execute_reply.started":"2023-02-25T17:35:06.290887Z","shell.execute_reply":"2023-02-25T17:35:06.303479Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport pyarrow as pa\nfrom datasets import Dataset\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import DebertaV2ForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\n\n\nfor model_name, val in  models.items():\n    df = pd.read_csv('/kaggle/input/semeval2023-task10/train_all_tasks.csv')[[\"text\", \"label_sexist\"]]\n\n    tokenizer = AutoTokenizer.from_pretrained(val[\"model\"])\n    if tokenizer.pad_token is None:\n        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n    processed_data = []\n    for i in range(len(df)):\n        processed_data.append(process_data(df.iloc[i], val[\"max_len\"]))\n    train_df = pd.DataFrame(processed_data)\n    train_hg = Dataset(pa.Table.from_pandas(train_df))\n\n    model = AutoModelForSequenceClassification.from_pretrained(val[\"model\"], num_labels=2, ignore_mismatched_sizes=True)\n\n    training_args = TrainingArguments(output_dir=\"./result\", evaluation_strategy=\"no\",\n                                      num_train_epochs=3, learning_rate = val[\"learning_rate\"], per_device_train_batch_size=val[\"batch_size\"])\n    trainer = Trainer(model=model,args=training_args,train_dataset=train_hg,tokenizer=tokenizer)\n    trainer.train()\n\n    device = torch.device(\"cuda\")\n    model.cuda()\n    get_probabilites_dev(model_name, val)\n    get_probabilites_test(model_name, val)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:35:07.853708Z","iopub.execute_input":"2023-02-25T17:35:07.854847Z","iopub.status.idle":"2023-02-25T21:10:56.393461Z","shell.execute_reply.started":"2023-02-25T17:35:07.854773Z","shell.execute_reply":"2023-02-25T21:10:56.391168Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":75,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/ec748fd4f03d0e5a2d5d56dff01e6dd733f23c67105cd54a9910f9d711870253.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/967a4d63eb35950cfd24a9e335906419009f32940fa2ba1b73e7ba032628c38d.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nAdding [MASK] to the vocabulary\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\n/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/322954555e18d37b9f98196875590497a8b33244551ec195e00fcdb831878224.f80cec19a933132f8b9636fbb3b736b2f4b9c8deaa45dfe4dd6e40a4d9970f44\nSome weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 05:51, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.366300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.213300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:53<00:00, 37.34it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:47<00:00, 37.29it/s]\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/ec748fd4f03d0e5a2d5d56dff01e6dd733f23c67105cd54a9910f9d711870253.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/967a4d63eb35950cfd24a9e335906419009f32940fa2ba1b73e7ba032628c38d.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nAdding [MASK] to the vocabulary\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\n/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/322954555e18d37b9f98196875590497a8b33244551ec195e00fcdb831878224.f80cec19a933132f8b9636fbb3b736b2f4b9c8deaa45dfe4dd6e40a4d9970f44\nSome weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 06:32, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.367700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.217700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:54<00:00, 36.81it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:47<00:00, 37.08it/s]\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/ec748fd4f03d0e5a2d5d56dff01e6dd733f23c67105cd54a9910f9d711870253.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/967a4d63eb35950cfd24a9e335906419009f32940fa2ba1b73e7ba032628c38d.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nAdding [MASK] to the vocabulary\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\n/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/322954555e18d37b9f98196875590497a8b33244551ec195e00fcdb831878224.f80cec19a933132f8b9636fbb3b736b2f4b9c8deaa45dfe4dd6e40a4d9970f44\nSome weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 07:31, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.366300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.225300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:57<00:00, 34.49it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:53<00:00, 35.27it/s]\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/ec748fd4f03d0e5a2d5d56dff01e6dd733f23c67105cd54a9910f9d711870253.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/967a4d63eb35950cfd24a9e335906419009f32940fa2ba1b73e7ba032628c38d.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nAdding [MASK] to the vocabulary\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\n/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/322954555e18d37b9f98196875590497a8b33244551ec195e00fcdb831878224.f80cec19a933132f8b9636fbb3b736b2f4b9c8deaa45dfe4dd6e40a4d9970f44\nSome weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 08:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.363200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.223400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:57<00:00, 34.98it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:53<00:00, 35.11it/s]\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/ec748fd4f03d0e5a2d5d56dff01e6dd733f23c67105cd54a9910f9d711870253.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/967a4d63eb35950cfd24a9e335906419009f32940fa2ba1b73e7ba032628c38d.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nAdding [MASK] to the vocabulary\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\n/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/322954555e18d37b9f98196875590497a8b33244551ec195e00fcdb831878224.f80cec19a933132f8b9636fbb3b736b2f4b9c8deaa45dfe4dd6e40a4d9970f44\nSome weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 09:08, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.369500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.207900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:57<00:00, 34.94it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:54<00:00, 35.01it/s]\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/ec748fd4f03d0e5a2d5d56dff01e6dd733f23c67105cd54a9910f9d711870253.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/967a4d63eb35950cfd24a9e335906419009f32940fa2ba1b73e7ba032628c38d.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nAdding [MASK] to the vocabulary\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\n/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/322954555e18d37b9f98196875590497a8b33244551ec195e00fcdb831878224.f80cec19a933132f8b9636fbb3b736b2f4b9c8deaa45dfe4dd6e40a4d9970f44\nSome weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 09:40, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.368700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.216100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:58<00:00, 34.30it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:55<00:00, 34.52it/s]\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/ec748fd4f03d0e5a2d5d56dff01e6dd733f23c67105cd54a9910f9d711870253.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/967a4d63eb35950cfd24a9e335906419009f32940fa2ba1b73e7ba032628c38d.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nAdding [MASK] to the vocabulary\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\n/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\nModel config DebertaV2Config {\n  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/322954555e18d37b9f98196875590497a8b33244551ec195e00fcdb831878224.f80cec19a933132f8b9636fbb3b736b2f4b9c8deaa45dfe4dd6e40a4d9970f44\nSome weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 10:44, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.365800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.221000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:56<00:00, 35.13it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:52<00:00, 35.44it/s]\nhttps://huggingface.co/MutazYoune/bert_racism3/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpoxc65gpc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87c66889b49e4fdb93aef37104cb9b63"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism3/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/0c1d6c067c8dd6637b157574e10921ca50e1cfdf22b103f710ed3c00c1d65687.a8dad7012e33d52ce4f563dabf7f8c83c2500373812fbb8e4268b4daaf8372b0\ncreating metadata file for /root/.cache/huggingface/transformers/0c1d6c067c8dd6637b157574e10921ca50e1cfdf22b103f710ed3c00c1d65687.a8dad7012e33d52ce4f563dabf7f8c83c2500373812fbb8e4268b4daaf8372b0\nhttps://huggingface.co/MutazYoune/bert_racism3/resolve/main/spm.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgj3k2lav\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c06efeb397a43b399eb3202f18d950c"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism3/resolve/main/spm.model in cache at /root/.cache/huggingface/transformers/1b77b66f69658f6fd2a6c6907c98f0ec5a68563baabe91fe1bff7bd5c69fcc74.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\ncreating metadata file for /root/.cache/huggingface/transformers/1b77b66f69658f6fd2a6c6907c98f0ec5a68563baabe91fe1bff7bd5c69fcc74.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nhttps://huggingface.co/MutazYoune/bert_racism3/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpohe_qme5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/8.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c4e4a7fe824400ab2f2fb2dc2ee55b3"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism3/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/951a46f5daa34ed391a7771c6d04d8618f1c89f4b72c85c33cf9fc33598aaf0e.3b6d8942356b992bfdee771dfbc3ba30350e8253bf7f5788589f60ca385b230b\ncreating metadata file for /root/.cache/huggingface/transformers/951a46f5daa34ed391a7771c6d04d8618f1c89f4b72c85c33cf9fc33598aaf0e.3b6d8942356b992bfdee771dfbc3ba30350e8253bf7f5788589f60ca385b230b\nhttps://huggingface.co/MutazYoune/bert_racism3/resolve/main/added_tokens.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptes9hfqz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/23.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b3e7f09220a430385e0ac03ef8982d1"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism3/resolve/main/added_tokens.json in cache at /root/.cache/huggingface/transformers/6c2a93b5e9995eb71df4548f5dea62ec6c91330e8f7e7fb9a4dcca8c22337c2f.5c109f2a5aa8f818a498d05f8e8ba1f2db4a07d9f2432713f54dcdc65e7c517b\ncreating metadata file for /root/.cache/huggingface/transformers/6c2a93b5e9995eb71df4548f5dea62ec6c91330e8f7e7fb9a4dcca8c22337c2f.5c109f2a5aa8f818a498d05f8e8ba1f2db4a07d9f2432713f54dcdc65e7c517b\nhttps://huggingface.co/MutazYoune/bert_racism3/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp950upu92\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5612204277c4c89b540646bc257fe67"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism3/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/cc0d9c97d97c164688b1d1805804819ca3b64dcdfea774848422ca9192025dc8.33ea9968fa7ac107543fcd0be7da0a5b475c882cb5390b8ce2fc4a0c34fe4d6b\ncreating metadata file for /root/.cache/huggingface/transformers/cc0d9c97d97c164688b1d1805804819ca3b64dcdfea774848422ca9192025dc8.33ea9968fa7ac107543fcd0be7da0a5b475c882cb5390b8ce2fc4a0c34fe4d6b\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/1b77b66f69658f6fd2a6c6907c98f0ec5a68563baabe91fe1bff7bd5c69fcc74.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/951a46f5daa34ed391a7771c6d04d8618f1c89f4b72c85c33cf9fc33598aaf0e.3b6d8942356b992bfdee771dfbc3ba30350e8253bf7f5788589f60ca385b230b\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/6c2a93b5e9995eb71df4548f5dea62ec6c91330e8f7e7fb9a4dcca8c22337c2f.5c109f2a5aa8f818a498d05f8e8ba1f2db4a07d9f2432713f54dcdc65e7c517b\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/cc0d9c97d97c164688b1d1805804819ca3b64dcdfea774848422ca9192025dc8.33ea9968fa7ac107543fcd0be7da0a5b475c882cb5390b8ce2fc4a0c34fe4d6b\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/0c1d6c067c8dd6637b157574e10921ca50e1cfdf22b103f710ed3c00c1d65687.a8dad7012e33d52ce4f563dabf7f8c83c2500373812fbb8e4268b4daaf8372b0\nhttps://huggingface.co/MutazYoune/bert_racism3/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjg6uduta\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/871 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b924255cf2b4d59b30a6d9721abf77f"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism3/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/df768000214520cf6f01e053367b1da8842422ee9b3433a8e929c955c15a5fc6.1906c5191734459e9c9b1145adc81be71d2606d32f6820a4492e27ab130bf935\ncreating metadata file for /root/.cache/huggingface/transformers/df768000214520cf6f01e053367b1da8842422ee9b3433a8e929c955c15a5fc6.1906c5191734459e9c9b1145adc81be71d2606d32f6820a4492e27ab130bf935\nloading configuration file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/df768000214520cf6f01e053367b1da8842422ee9b3433a8e929c955c15a5fc6.1906c5191734459e9c9b1145adc81be71d2606d32f6820a4492e27ab130bf935\nModel config DebertaV2Config {\n  \"_name_or_path\": \"MutazYoune/bert_racism3\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nhttps://huggingface.co/MutazYoune/bert_racism3/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpoltk33q4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/704M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9789befc7da947bbb33afc753e983846"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism3/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/087bdb21fe7b3a10a0e98e75229609d2ff2b5d1536292c072fecceb31c9c8a59.9ff43e47a50c46da33456fe9cfb1cf2b846686232411f25a114682c43307c23c\ncreating metadata file for /root/.cache/huggingface/transformers/087bdb21fe7b3a10a0e98e75229609d2ff2b5d1536292c072fecceb31c9c8a59.9ff43e47a50c46da33456fe9cfb1cf2b846686232411f25a114682c43307c23c\nloading weights file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/087bdb21fe7b3a10a0e98e75229609d2ff2b5d1536292c072fecceb31c9c8a59.9ff43e47a50c46da33456fe9cfb1cf2b846686232411f25a114682c43307c23c\nSome weights of the model checkpoint at MutazYoune/bert_racism3 were not used when initializing DebertaV2ForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at MutazYoune/bert_racism3 and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 08:45, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.367700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.236400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:56<00:00, 35.53it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:51<00:00, 35.79it/s]\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/1b77b66f69658f6fd2a6c6907c98f0ec5a68563baabe91fe1bff7bd5c69fcc74.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/951a46f5daa34ed391a7771c6d04d8618f1c89f4b72c85c33cf9fc33598aaf0e.3b6d8942356b992bfdee771dfbc3ba30350e8253bf7f5788589f60ca385b230b\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/6c2a93b5e9995eb71df4548f5dea62ec6c91330e8f7e7fb9a4dcca8c22337c2f.5c109f2a5aa8f818a498d05f8e8ba1f2db4a07d9f2432713f54dcdc65e7c517b\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/cc0d9c97d97c164688b1d1805804819ca3b64dcdfea774848422ca9192025dc8.33ea9968fa7ac107543fcd0be7da0a5b475c882cb5390b8ce2fc4a0c34fe4d6b\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/0c1d6c067c8dd6637b157574e10921ca50e1cfdf22b103f710ed3c00c1d65687.a8dad7012e33d52ce4f563dabf7f8c83c2500373812fbb8e4268b4daaf8372b0\nloading configuration file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/df768000214520cf6f01e053367b1da8842422ee9b3433a8e929c955c15a5fc6.1906c5191734459e9c9b1145adc81be71d2606d32f6820a4492e27ab130bf935\nModel config DebertaV2Config {\n  \"_name_or_path\": \"MutazYoune/bert_racism3\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/087bdb21fe7b3a10a0e98e75229609d2ff2b5d1536292c072fecceb31c9c8a59.9ff43e47a50c46da33456fe9cfb1cf2b846686232411f25a114682c43307c23c\nSome weights of the model checkpoint at MutazYoune/bert_racism3 were not used when initializing DebertaV2ForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at MutazYoune/bert_racism3 and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 09:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.367100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.229300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:56<00:00, 35.22it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:54<00:00, 34.88it/s]\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/1b77b66f69658f6fd2a6c6907c98f0ec5a68563baabe91fe1bff7bd5c69fcc74.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/951a46f5daa34ed391a7771c6d04d8618f1c89f4b72c85c33cf9fc33598aaf0e.3b6d8942356b992bfdee771dfbc3ba30350e8253bf7f5788589f60ca385b230b\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/6c2a93b5e9995eb71df4548f5dea62ec6c91330e8f7e7fb9a4dcca8c22337c2f.5c109f2a5aa8f818a498d05f8e8ba1f2db4a07d9f2432713f54dcdc65e7c517b\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/cc0d9c97d97c164688b1d1805804819ca3b64dcdfea774848422ca9192025dc8.33ea9968fa7ac107543fcd0be7da0a5b475c882cb5390b8ce2fc4a0c34fe4d6b\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/0c1d6c067c8dd6637b157574e10921ca50e1cfdf22b103f710ed3c00c1d65687.a8dad7012e33d52ce4f563dabf7f8c83c2500373812fbb8e4268b4daaf8372b0\nloading configuration file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/df768000214520cf6f01e053367b1da8842422ee9b3433a8e929c955c15a5fc6.1906c5191734459e9c9b1145adc81be71d2606d32f6820a4492e27ab130bf935\nModel config DebertaV2Config {\n  \"_name_or_path\": \"MutazYoune/bert_racism3\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/087bdb21fe7b3a10a0e98e75229609d2ff2b5d1536292c072fecceb31c9c8a59.9ff43e47a50c46da33456fe9cfb1cf2b846686232411f25a114682c43307c23c\nSome weights of the model checkpoint at MutazYoune/bert_racism3 were not used when initializing DebertaV2ForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at MutazYoune/bert_racism3 and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 09:41, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.356300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.229700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:57<00:00, 34.56it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:54<00:00, 34.81it/s]\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/1b77b66f69658f6fd2a6c6907c98f0ec5a68563baabe91fe1bff7bd5c69fcc74.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/951a46f5daa34ed391a7771c6d04d8618f1c89f4b72c85c33cf9fc33598aaf0e.3b6d8942356b992bfdee771dfbc3ba30350e8253bf7f5788589f60ca385b230b\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/6c2a93b5e9995eb71df4548f5dea62ec6c91330e8f7e7fb9a4dcca8c22337c2f.5c109f2a5aa8f818a498d05f8e8ba1f2db4a07d9f2432713f54dcdc65e7c517b\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/cc0d9c97d97c164688b1d1805804819ca3b64dcdfea774848422ca9192025dc8.33ea9968fa7ac107543fcd0be7da0a5b475c882cb5390b8ce2fc4a0c34fe4d6b\nloading file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/0c1d6c067c8dd6637b157574e10921ca50e1cfdf22b103f710ed3c00c1d65687.a8dad7012e33d52ce4f563dabf7f8c83c2500373812fbb8e4268b4daaf8372b0\nloading configuration file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/df768000214520cf6f01e053367b1da8842422ee9b3433a8e929c955c15a5fc6.1906c5191734459e9c9b1145adc81be71d2606d32f6820a4492e27ab130bf935\nModel config DebertaV2Config {\n  \"_name_or_path\": \"MutazYoune/bert_racism3\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/MutazYoune/bert_racism3/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/087bdb21fe7b3a10a0e98e75229609d2ff2b5d1536292c072fecceb31c9c8a59.9ff43e47a50c46da33456fe9cfb1cf2b846686232411f25a114682c43307c23c\nSome weights of the model checkpoint at MutazYoune/bert_racism3 were not used when initializing DebertaV2ForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at MutazYoune/bert_racism3 and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 10:39, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.360400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.228000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:57<00:00, 34.56it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:56<00:00, 34.47it/s]\nhttps://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpl5dcxh31\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62f0f966058c4db6b92344afa33e540b"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/3d60e95ba66329a36917a0585320784aab34c0b49e073283222d8606d87d926a.a8dad7012e33d52ce4f563dabf7f8c83c2500373812fbb8e4268b4daaf8372b0\ncreating metadata file for /root/.cache/huggingface/transformers/3d60e95ba66329a36917a0585320784aab34c0b49e073283222d8606d87d926a.a8dad7012e33d52ce4f563dabf7f8c83c2500373812fbb8e4268b4daaf8372b0\nhttps://huggingface.co/MutazYoune/bert_racism4/resolve/main/spm.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp064ktw9g\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4ce512ee8164810960e3cd86d4a2507"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism4/resolve/main/spm.model in cache at /root/.cache/huggingface/transformers/776608143b227824791019cc01fecdd8ffc1e991f2d97f2f3872cfb470fcc07b.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\ncreating metadata file for /root/.cache/huggingface/transformers/776608143b227824791019cc01fecdd8ffc1e991f2d97f2f3872cfb470fcc07b.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nhttps://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp67zvl1ok\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/8.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6cf382e800c49bca193c33bf164a756"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/3af3491885f5ce32abc72d834e9c3856821869d1c0277cf9eba212515ca10c4a.3b6d8942356b992bfdee771dfbc3ba30350e8253bf7f5788589f60ca385b230b\ncreating metadata file for /root/.cache/huggingface/transformers/3af3491885f5ce32abc72d834e9c3856821869d1c0277cf9eba212515ca10c4a.3b6d8942356b992bfdee771dfbc3ba30350e8253bf7f5788589f60ca385b230b\nhttps://huggingface.co/MutazYoune/bert_racism4/resolve/main/added_tokens.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp86i2xcra\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/23.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c26c903da1140a09d8535bc5a9979da"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism4/resolve/main/added_tokens.json in cache at /root/.cache/huggingface/transformers/1e96b4e1441f3df8e98f6c779e770d50a1d34815fba28d131d05820c73536a54.5c109f2a5aa8f818a498d05f8e8ba1f2db4a07d9f2432713f54dcdc65e7c517b\ncreating metadata file for /root/.cache/huggingface/transformers/1e96b4e1441f3df8e98f6c779e770d50a1d34815fba28d131d05820c73536a54.5c109f2a5aa8f818a498d05f8e8ba1f2db4a07d9f2432713f54dcdc65e7c517b\nhttps://huggingface.co/MutazYoune/bert_racism4/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvs60vlua\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c5d37a08e894ae5ae9766e57ee58cdf"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism4/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/795a03c3dc68cd437e47f00bd6b3fbb97689d03502af59bfd3393db5f57cf963.33ea9968fa7ac107543fcd0be7da0a5b475c882cb5390b8ce2fc4a0c34fe4d6b\ncreating metadata file for /root/.cache/huggingface/transformers/795a03c3dc68cd437e47f00bd6b3fbb97689d03502af59bfd3393db5f57cf963.33ea9968fa7ac107543fcd0be7da0a5b475c882cb5390b8ce2fc4a0c34fe4d6b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/776608143b227824791019cc01fecdd8ffc1e991f2d97f2f3872cfb470fcc07b.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/3af3491885f5ce32abc72d834e9c3856821869d1c0277cf9eba212515ca10c4a.3b6d8942356b992bfdee771dfbc3ba30350e8253bf7f5788589f60ca385b230b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/1e96b4e1441f3df8e98f6c779e770d50a1d34815fba28d131d05820c73536a54.5c109f2a5aa8f818a498d05f8e8ba1f2db4a07d9f2432713f54dcdc65e7c517b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/795a03c3dc68cd437e47f00bd6b3fbb97689d03502af59bfd3393db5f57cf963.33ea9968fa7ac107543fcd0be7da0a5b475c882cb5390b8ce2fc4a0c34fe4d6b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/3d60e95ba66329a36917a0585320784aab34c0b49e073283222d8606d87d926a.a8dad7012e33d52ce4f563dabf7f8c83c2500373812fbb8e4268b4daaf8372b0\nhttps://huggingface.co/MutazYoune/bert_racism4/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpa3zwqko9\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/871 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6f56670e6b44ff6a6ae4a866134c324"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism4/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/9e7b0b1676ab68c5505fb45cb8dfdc266d8049284371c6445b891cca955a3c12.1906c5191734459e9c9b1145adc81be71d2606d32f6820a4492e27ab130bf935\ncreating metadata file for /root/.cache/huggingface/transformers/9e7b0b1676ab68c5505fb45cb8dfdc266d8049284371c6445b891cca955a3c12.1906c5191734459e9c9b1145adc81be71d2606d32f6820a4492e27ab130bf935\nloading configuration file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9e7b0b1676ab68c5505fb45cb8dfdc266d8049284371c6445b891cca955a3c12.1906c5191734459e9c9b1145adc81be71d2606d32f6820a4492e27ab130bf935\nModel config DebertaV2Config {\n  \"_name_or_path\": \"MutazYoune/bert_racism4\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nhttps://huggingface.co/MutazYoune/bert_racism4/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpw3s0pcb4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/704M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e434a8721f1b4f11a653a62aa5744e90"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_racism4/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0a16af4e04a2827d4c38e757e245ff3b51609740420d4b90702871ba064db5b6.756cfc5ab491d3267c5bc45b683d01904ed289bdc23f349dcc400afd2a4ff005\ncreating metadata file for /root/.cache/huggingface/transformers/0a16af4e04a2827d4c38e757e245ff3b51609740420d4b90702871ba064db5b6.756cfc5ab491d3267c5bc45b683d01904ed289bdc23f349dcc400afd2a4ff005\nloading weights file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a16af4e04a2827d4c38e757e245ff3b51609740420d4b90702871ba064db5b6.756cfc5ab491d3267c5bc45b683d01904ed289bdc23f349dcc400afd2a4ff005\nSome weights of the model checkpoint at MutazYoune/bert_racism4 were not used when initializing DebertaV2ForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at MutazYoune/bert_racism4 and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 07:30, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.361100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.226800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:57<00:00, 34.69it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:54<00:00, 34.87it/s]\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/776608143b227824791019cc01fecdd8ffc1e991f2d97f2f3872cfb470fcc07b.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/3af3491885f5ce32abc72d834e9c3856821869d1c0277cf9eba212515ca10c4a.3b6d8942356b992bfdee771dfbc3ba30350e8253bf7f5788589f60ca385b230b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/1e96b4e1441f3df8e98f6c779e770d50a1d34815fba28d131d05820c73536a54.5c109f2a5aa8f818a498d05f8e8ba1f2db4a07d9f2432713f54dcdc65e7c517b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/795a03c3dc68cd437e47f00bd6b3fbb97689d03502af59bfd3393db5f57cf963.33ea9968fa7ac107543fcd0be7da0a5b475c882cb5390b8ce2fc4a0c34fe4d6b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/3d60e95ba66329a36917a0585320784aab34c0b49e073283222d8606d87d926a.a8dad7012e33d52ce4f563dabf7f8c83c2500373812fbb8e4268b4daaf8372b0\nloading configuration file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9e7b0b1676ab68c5505fb45cb8dfdc266d8049284371c6445b891cca955a3c12.1906c5191734459e9c9b1145adc81be71d2606d32f6820a4492e27ab130bf935\nModel config DebertaV2Config {\n  \"_name_or_path\": \"MutazYoune/bert_racism4\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a16af4e04a2827d4c38e757e245ff3b51609740420d4b90702871ba064db5b6.756cfc5ab491d3267c5bc45b683d01904ed289bdc23f349dcc400afd2a4ff005\nSome weights of the model checkpoint at MutazYoune/bert_racism4 were not used when initializing DebertaV2ForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at MutazYoune/bert_racism4 and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 08:42, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.358900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.228200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:57<00:00, 34.73it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:54<00:00, 35.01it/s]\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/776608143b227824791019cc01fecdd8ffc1e991f2d97f2f3872cfb470fcc07b.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/3af3491885f5ce32abc72d834e9c3856821869d1c0277cf9eba212515ca10c4a.3b6d8942356b992bfdee771dfbc3ba30350e8253bf7f5788589f60ca385b230b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/1e96b4e1441f3df8e98f6c779e770d50a1d34815fba28d131d05820c73536a54.5c109f2a5aa8f818a498d05f8e8ba1f2db4a07d9f2432713f54dcdc65e7c517b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/795a03c3dc68cd437e47f00bd6b3fbb97689d03502af59bfd3393db5f57cf963.33ea9968fa7ac107543fcd0be7da0a5b475c882cb5390b8ce2fc4a0c34fe4d6b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/3d60e95ba66329a36917a0585320784aab34c0b49e073283222d8606d87d926a.a8dad7012e33d52ce4f563dabf7f8c83c2500373812fbb8e4268b4daaf8372b0\nloading configuration file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9e7b0b1676ab68c5505fb45cb8dfdc266d8049284371c6445b891cca955a3c12.1906c5191734459e9c9b1145adc81be71d2606d32f6820a4492e27ab130bf935\nModel config DebertaV2Config {\n  \"_name_or_path\": \"MutazYoune/bert_racism4\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a16af4e04a2827d4c38e757e245ff3b51609740420d4b90702871ba064db5b6.756cfc5ab491d3267c5bc45b683d01904ed289bdc23f349dcc400afd2a4ff005\nSome weights of the model checkpoint at MutazYoune/bert_racism4 were not used when initializing DebertaV2ForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at MutazYoune/bert_racism4 and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 09:18, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.361700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.229200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:58<00:00, 34.09it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [02:00<00:00, 33.14it/s]\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/776608143b227824791019cc01fecdd8ffc1e991f2d97f2f3872cfb470fcc07b.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/3af3491885f5ce32abc72d834e9c3856821869d1c0277cf9eba212515ca10c4a.3b6d8942356b992bfdee771dfbc3ba30350e8253bf7f5788589f60ca385b230b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/1e96b4e1441f3df8e98f6c779e770d50a1d34815fba28d131d05820c73536a54.5c109f2a5aa8f818a498d05f8e8ba1f2db4a07d9f2432713f54dcdc65e7c517b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/795a03c3dc68cd437e47f00bd6b3fbb97689d03502af59bfd3393db5f57cf963.33ea9968fa7ac107543fcd0be7da0a5b475c882cb5390b8ce2fc4a0c34fe4d6b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/3d60e95ba66329a36917a0585320784aab34c0b49e073283222d8606d87d926a.a8dad7012e33d52ce4f563dabf7f8c83c2500373812fbb8e4268b4daaf8372b0\nloading configuration file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9e7b0b1676ab68c5505fb45cb8dfdc266d8049284371c6445b891cca955a3c12.1906c5191734459e9c9b1145adc81be71d2606d32f6820a4492e27ab130bf935\nModel config DebertaV2Config {\n  \"_name_or_path\": \"MutazYoune/bert_racism4\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a16af4e04a2827d4c38e757e245ff3b51609740420d4b90702871ba064db5b6.756cfc5ab491d3267c5bc45b683d01904ed289bdc23f349dcc400afd2a4ff005\nSome weights of the model checkpoint at MutazYoune/bert_racism4 were not used when initializing DebertaV2ForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at MutazYoune/bert_racism4 and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 09:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.356100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.222500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [01:00<00:00, 33.09it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:58<00:00, 33.75it/s]\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/spm.model from cache at /root/.cache/huggingface/transformers/776608143b227824791019cc01fecdd8ffc1e991f2d97f2f3872cfb470fcc07b.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/3af3491885f5ce32abc72d834e9c3856821869d1c0277cf9eba212515ca10c4a.3b6d8942356b992bfdee771dfbc3ba30350e8253bf7f5788589f60ca385b230b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/1e96b4e1441f3df8e98f6c779e770d50a1d34815fba28d131d05820c73536a54.5c109f2a5aa8f818a498d05f8e8ba1f2db4a07d9f2432713f54dcdc65e7c517b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/795a03c3dc68cd437e47f00bd6b3fbb97689d03502af59bfd3393db5f57cf963.33ea9968fa7ac107543fcd0be7da0a5b475c882cb5390b8ce2fc4a0c34fe4d6b\nloading file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/3d60e95ba66329a36917a0585320784aab34c0b49e073283222d8606d87d926a.a8dad7012e33d52ce4f563dabf7f8c83c2500373812fbb8e4268b4daaf8372b0\nloading configuration file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9e7b0b1676ab68c5505fb45cb8dfdc266d8049284371c6445b891cca955a3c12.1906c5191734459e9c9b1145adc81be71d2606d32f6820a4492e27ab130bf935\nModel config DebertaV2Config {\n  \"_name_or_path\": \"MutazYoune/bert_racism4\",\n  \"architectures\": [\n    \"DebertaV2ForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-07,\n  \"max_position_embeddings\": 512,\n  \"max_relative_positions\": -1,\n  \"model_type\": \"deberta-v2\",\n  \"norm_rel_ebd\": \"layer_norm\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_dropout\": 0,\n  \"pooler_hidden_act\": \"gelu\",\n  \"pooler_hidden_size\": 768,\n  \"pos_att_type\": [\n    \"p2c\",\n    \"c2p\"\n  ],\n  \"position_biased_input\": false,\n  \"position_buckets\": 256,\n  \"relative_attention\": true,\n  \"share_att_key\": true,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 0,\n  \"vocab_size\": 128100\n}\n\nloading weights file https://huggingface.co/MutazYoune/bert_racism4/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a16af4e04a2827d4c38e757e245ff3b51609740420d4b90702871ba064db5b6.756cfc5ab491d3267c5bc45b683d01904ed289bdc23f349dcc400afd2a4ff005\nSome weights of the model checkpoint at MutazYoune/bert_racism4 were not used when initializing DebertaV2ForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at MutazYoune/bert_racism4 and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 10:47, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.372100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.239200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [01:00<00:00, 33.21it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [01:58<00:00, 33.70it/s]\nhttps://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphlgjdl5_\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/542 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"952366210aee47e3abdd324e21968021"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/25a79bf0239c19aff2a600d8669498ec942639071d8b75fbc33c895be64325e6.28026d2c019c14a9be40012100c25af113ce22b8f0e7bfed6e1f394f4b6ae52d\ncreating metadata file for /root/.cache/huggingface/transformers/25a79bf0239c19aff2a600d8669498ec942639071d8b75fbc33c895be64325e6.28026d2c019c14a9be40012100c25af113ce22b8f0e7bfed6e1f394f4b6ae52d\nhttps://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6hkn0szj\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65104c64c2ec4ea6b58319c3a3b7ea49"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/8e45d7578ba6e6a588268cc23bf3131f57b93daca32dfb6751fb03621cef966d.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\ncreating metadata file for /root/.cache/huggingface/transformers/8e45d7578ba6e6a588268cc23bf3131f57b93daca32dfb6751fb03621cef966d.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nhttps://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbb9eppf4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09afc6799bd34b7487fe74bf18c93c02"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/b0d53fe906f61a06539a1a5bfa93d39536fd33c9903ca65ad17f1c7d3465ea40.7da70648c6cb9951e284c9685f9ba7ae083dd59ed1d6d84bdfc0584a4ea94b6d\ncreating metadata file for /root/.cache/huggingface/transformers/b0d53fe906f61a06539a1a5bfa93d39536fd33c9903ca65ad17f1c7d3465ea40.7da70648c6cb9951e284c9685f9ba7ae083dd59ed1d6d84bdfc0584a4ea94b6d\nloading file https://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/8e45d7578ba6e6a588268cc23bf3131f57b93daca32dfb6751fb03621cef966d.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nloading file https://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/b0d53fe906f61a06539a1a5bfa93d39536fd33c9903ca65ad17f1c7d3465ea40.7da70648c6cb9951e284c9685f9ba7ae083dd59ed1d6d84bdfc0584a4ea94b6d\nloading file https://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/25a79bf0239c19aff2a600d8669498ec942639071d8b75fbc33c895be64325e6.28026d2c019c14a9be40012100c25af113ce22b8f0e7bfed6e1f394f4b6ae52d\nhttps://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpicoq0tli\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/675 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cd44341f45c49b1b5dadb907e3cfcf6"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/60fc1193e5f35d8fde8d1d492470935291c6bd301c600d4e3470f4555d537283.fd4c182e8a9887af68c7d58346c45310401a9872ddf9142878704bda90be4f17\ncreating metadata file for /root/.cache/huggingface/transformers/60fc1193e5f35d8fde8d1d492470935291c6bd301c600d4e3470f4555d537283.fd4c182e8a9887af68c7d58346c45310401a9872ddf9142878704bda90be4f17\nloading configuration file https://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/60fc1193e5f35d8fde8d1d492470935291c6bd301c600d4e3470f4555d537283.fd4c182e8a9887af68c7d58346c45310401a9872ddf9142878704bda90be4f17\nModel config BertConfig {\n  \"_name_or_path\": \"MutazYoune/bert_hateracism90000\",\n  \"_num_labels\": 2,\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nhttps://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpu6wzhqwi\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95703b88a5e54e8dae2d9b26f49b4b54"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/aa4653bdfc067454dd7a1e475d12b74e22c735a886901c3139c08ea28b54c9c4.e575f209167c60be2cad5f8143a3a29a463811436cd6db0412007cb858e67d59\ncreating metadata file for /root/.cache/huggingface/transformers/aa4653bdfc067454dd7a1e475d12b74e22c735a886901c3139c08ea28b54c9c4.e575f209167c60be2cad5f8143a3a29a463811436cd6db0412007cb858e67d59\nloading weights file https://huggingface.co/MutazYoune/bert_hateracism90000/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/aa4653bdfc067454dd7a1e475d12b74e22c735a886901c3139c08ea28b54c9c4.e575f209167c60be2cad5f8143a3a29a463811436cd6db0412007cb858e67d59\nSome weights of the model checkpoint at MutazYoune/bert_hateracism90000 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at MutazYoune/bert_hateracism90000 and are newly initialized: ['classifier.weight', 'classifier.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 06:39, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.311100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.142600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:26<00:00, 76.26it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [00:51<00:00, 77.48it/s]\nhttps://huggingface.co/GroNLP/hateBERT/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp45lcqofg\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/151 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a29b0e59fde4ec58961c59b2fbd436b"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/GroNLP/hateBERT/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/614804c7446f9112c395c092683d4a920be32da50c7a6d9c4efe82d116700304.2e7e43ed2f73736f81cc767c6251e910afbdff82fdf510fece2e7da58c6e7b5c\ncreating metadata file for /root/.cache/huggingface/transformers/614804c7446f9112c395c092683d4a920be32da50c7a6d9c4efe82d116700304.2e7e43ed2f73736f81cc767c6251e910afbdff82fdf510fece2e7da58c6e7b5c\nhttps://huggingface.co/GroNLP/hateBERT/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp40kbfsvz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb1eba843a5648f78b8cca8448119a98"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/GroNLP/hateBERT/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/beaf3bf77311e5d01ddc7e50c577f9c2a26c55a8ec8ea5f8c81f755acdeebc10.53b298b0646108c5ea759a24a4d397baeea68c6d2844015cb0a6f6d4eda0cb67\ncreating metadata file for /root/.cache/huggingface/transformers/beaf3bf77311e5d01ddc7e50c577f9c2a26c55a8ec8ea5f8c81f755acdeebc10.53b298b0646108c5ea759a24a4d397baeea68c6d2844015cb0a6f6d4eda0cb67\nloading configuration file https://huggingface.co/GroNLP/hateBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/beaf3bf77311e5d01ddc7e50c577f9c2a26c55a8ec8ea5f8c81f755acdeebc10.53b298b0646108c5ea759a24a4d397baeea68c6d2844015cb0a6f6d4eda0cb67\nModel config BertConfig {\n  \"_name_or_path\": \"GroNLP/hateBERT\",\n  \"_num_labels\": 2,\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nhttps://huggingface.co/GroNLP/hateBERT/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpa3wsuwnj\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1c7d7b608e74426a02658e08980e874"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/GroNLP/hateBERT/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/49d465421d47e594ee91d545384e05f9983710fbcf60a70ee1eedda2f9421d3d.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\ncreating metadata file for /root/.cache/huggingface/transformers/49d465421d47e594ee91d545384e05f9983710fbcf60a70ee1eedda2f9421d3d.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nhttps://huggingface.co/GroNLP/hateBERT/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdredwvls\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b89e941a463841388252ce32539a26a2"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/GroNLP/hateBERT/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/11f5da32001370f4754cfc3a58f35f0f4d957be3460bfa723d54f2a723b310f3.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\ncreating metadata file for /root/.cache/huggingface/transformers/11f5da32001370f4754cfc3a58f35f0f4d957be3460bfa723d54f2a723b310f3.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\nloading file https://huggingface.co/GroNLP/hateBERT/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/49d465421d47e594ee91d545384e05f9983710fbcf60a70ee1eedda2f9421d3d.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nloading file https://huggingface.co/GroNLP/hateBERT/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/GroNLP/hateBERT/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/GroNLP/hateBERT/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/11f5da32001370f4754cfc3a58f35f0f4d957be3460bfa723d54f2a723b310f3.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\nloading file https://huggingface.co/GroNLP/hateBERT/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/614804c7446f9112c395c092683d4a920be32da50c7a6d9c4efe82d116700304.2e7e43ed2f73736f81cc767c6251e910afbdff82fdf510fece2e7da58c6e7b5c\nloading configuration file https://huggingface.co/GroNLP/hateBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/beaf3bf77311e5d01ddc7e50c577f9c2a26c55a8ec8ea5f8c81f755acdeebc10.53b298b0646108c5ea759a24a4d397baeea68c6d2844015cb0a6f6d4eda0cb67\nModel config BertConfig {\n  \"_name_or_path\": \"GroNLP/hateBERT\",\n  \"_num_labels\": 2,\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading configuration file https://huggingface.co/GroNLP/hateBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/beaf3bf77311e5d01ddc7e50c577f9c2a26c55a8ec8ea5f8c81f755acdeebc10.53b298b0646108c5ea759a24a4d397baeea68c6d2844015cb0a6f6d4eda0cb67\nModel config BertConfig {\n  \"_name_or_path\": \"GroNLP/hateBERT\",\n  \"_num_labels\": 2,\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading configuration file https://huggingface.co/GroNLP/hateBERT/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/beaf3bf77311e5d01ddc7e50c577f9c2a26c55a8ec8ea5f8c81f755acdeebc10.53b298b0646108c5ea759a24a4d397baeea68c6d2844015cb0a6f6d4eda0cb67\nModel config BertConfig {\n  \"_name_or_path\": \"GroNLP/hateBERT\",\n  \"_num_labels\": 2,\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.20.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nhttps://huggingface.co/GroNLP/hateBERT/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpgmvu3fcn\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ff93ab2bcf947f3a0d3ce41f30b9143"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/GroNLP/hateBERT/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/2d60dac9cab334b55695ad87a4ab6ec3f4a6997076bdf05c35e02244152f20d6.f37afc087294d5ecdc0a69dd12092647a6e439435e9a0a4a790b086adb5b05f6\ncreating metadata file for /root/.cache/huggingface/transformers/2d60dac9cab334b55695ad87a4ab6ec3f4a6997076bdf05c35e02244152f20d6.f37afc087294d5ecdc0a69dd12092647a6e439435e9a0a4a790b086adb5b05f6\nloading weights file https://huggingface.co/GroNLP/hateBERT/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/2d60dac9cab334b55695ad87a4ab6ec3f4a6997076bdf05c35e02244152f20d6.f37afc087294d5ecdc0a69dd12092647a6e439435e9a0a4a790b086adb5b05f6\nSome weights of the model checkpoint at GroNLP/hateBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nThe following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 14000\n  Num Epochs = 3\n  Instantaneous batch size per device = 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1314\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1314/1314 06:38, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.324700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.137200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./result/checkpoint-500\nConfiguration saved in ./result/checkpoint-500/config.json\nModel weights saved in ./result/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to ./result/checkpoint-1000\nConfiguration saved in ./result/checkpoint-1000/config.json\nModel weights saved in ./result/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 2000/2000 [00:25<00:00, 77.47it/s]\n  0%|          | 0/4000 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  from ipykernel import kernelapp as app\n100%|██████████| 4000/4000 [00:51<00:00, 77.60it/s]\n","output_type":"stream"}]}]}